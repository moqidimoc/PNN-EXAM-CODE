# Karhunen-LoÃ¨ve Transform (KLT) -> For PCA 

# INPUT:
    # data: pd.DataFrame with instances as rows and features as columns

def karhunen(data):
    
    # First we calculate the mean of each column
    means = np.zeros(len(data.columns))# for all columns
    for i in range(len(data.columns)):
        means[i] = data.iloc[:, i].mean()
    
    mean_zero = np.zeros(data.shape)
    # Now we substract the corresponding mean to all the values of our dataset
    for j in range(len(data)): # for all instances
       mean_zero[j, :] = data.iloc[j, :] - means
        
    # Now we have to calculate the covariance matrix
    cov = np.zeros((len(data.columns), len(data.columns)))
    for i in range(len(data)): # for all instances
        cov += np.array([mean_zero[i, :]]).T*mean_zero[i, :]
    cov = cov/len(data) # This is our final covariance matrix
    
    # Lastly, we obtain the eigenvalues and its corresponding eigenvectors
    eigenVal, eigenVec = np.linalg.eig(cov)
    
    return means, cov, eigenVal, eigenVec
    
    
# Example from the Coursework 7 with the iris dataset

from sklearn import datasets
iris = datasets.load_iris()

# As our function performs on dataframes and numpy arrays, we will adapt this info to do so
irisdf = pd.DataFrame(iris.data)

means, cov, eigenVal, eigenVec = karhunen(irisdf)

print("EigenValues: {}\nEigenVectors: \n{}".format(eigenVal, eigenVec))

# If we pick the two first principal components we have:
val = eigenVal[:2]
v = eigenVec[:, :2]
print("\n\nEigenValues for the 2 first principal components: {}\nEigenVectors for the 2 first principal components: \n{}".format(val, v))

# Now, for a point you use: 
p1 = np.array([[5.6,3.6,3.1,0.3]])
y1 = np.dot(v.T, (p1 - means).T)
print("\n\nPoint before PCA: {}\nProjection into the two first principal components: \n{}".format(p1, y1))
