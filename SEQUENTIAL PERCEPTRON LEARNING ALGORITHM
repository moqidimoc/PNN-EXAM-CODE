# Sequential Perceptron Learning Algorithm

# INPUTS:
    # x: data
    # y: target
    # a: initial weights
    # eta: learning rate
    # n_epochs: number of epochs of learning on the entire dataset
    # augmentation: True if data is augmented
    # sample_normalisation: True if sample normalisation applied

def sequential_perceptron_learning(x, y, a, eta, n_epochs, augmentation, sample_normalisation):
    n_instances = len(x)
    
    if augmentation:
        x = np.insert(x, 0, 1, axis=1)
    
    if sample_normalisation: # CAREFUL WITH THE DATA LABEL
        x[y==-1] = -x[y==-1]
    
    if sample_normalisation:
        for n in range(n_epochs):
            print("Epoch #{}: ".format(n+1))
            for i in range(n_instances):
                if np.sign(np.dot(a, x[i])) <= 0: # only misclassified samples
                    a += eta*x[i]
                print("Sample #{}: {}".format(i+1, a))
            print("End of epoch #{}\n".format(n+1))


    else:
        for n in range(n_epochs):
            print("Epoch #{}: ".format(n+1))
            for i in range(n_instances):
                if np.sign(np.dot(a, x[i])) != y[i]: # only misclassified samples
                    a += eta*y[i]*x[i]
                print("Sample #{}: {}".format(i+1, a))
            print("End of epoch #{}\n".format(n+1))

    return a
    
    
# Tutorial example (exercise 7)
x = np.array([[1, 5], [2, 5], [4, 1], [5, 1]]) # data
y = np.array([1, 1, -1, -1]) # targets
a = np.array([-25, 6, 3]) # initial weights

# And here we estimate the new vector
a = sequential_perceptron_learning(x, y, a, 1, 3, True, True)
print(a) # It should be [-25   3   7]
