# Sequential Widrow-Hoff Learning Algorithm

# INPUTS:
    # x: data
    # y: target
    # a: initial weights
    # b: margin
    # eta: learning rate
    # n_epochs: number of epochs of learning on the entire dataset
    # augmentation: True if data is augmented
    # sample_normalisation: True if sample normalisation applied
    
def sequential_widrow(x, y, a, b, eta, n_epochs, augmentation, sample_normalisation):
    n_instances = len(x)
    
    if augmentation:
        x = np.insert(x, 0, 1, axis=1)
    
    if sample_normalisation: # CAREFUL WITH THE DATA LABEL
        x[y==-1] = -x[y==-1]
    
    for n in range(n_epochs):
        print("Epoch #{}: ".format(n+1))
        last_a_epoch = a
        for i in range(n_instances):            
            print("Iteration #{}: aTyK = {} // a = {}".format(n_instances*n+i+1,np.dot(a, x[i]), a + eta*(b[i] - np.dot(a, x[i]))*x[i]))
                     
            # Now we update a with its new value
            a += eta*(b[i] - np.dot(a, x[i]))*x[i]
                        
        # And before exiting the first loop, we print the new values after the first epoch
        print('Epoch #{}: last a: {} \nnew a: {}'.format(n+1, last_a_epoch, a))
        
    return a 
    
# Tutorial example (exercise 14)
x = np.array([[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]]) # data
y = np.array([1, 1, 1, -1, -1, -1]) # targets
a = np.array([1., 0., 0.]) # initial weights
b = np.array([1]*6) # margin

# And here we estimate the new vector
a = sequential_widrow(x, y, a, b, 0.1, 2, True, True)
print(a) # It should be [0.597 0.6259 -0.1715]
